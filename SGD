import numpy as np
class Module:
    def sgd_step(self, lrate): pass  # For modules w/o weights

class Linear(Module):
    def __init__(self, m, n):
        # initializes the weights randomly and offsets as 0
        self.m, self.n = (m, n)  # (in size, out size)
        self.W0 = np.zeros([self.n, 1])  # (n x 1)
        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)

    def forward(self, A):
        # store the input matrix for future use
        self.A = A   # (m x b)  Hint: make sure you understand what b stands for
        return np.dot(self.W.T,self.A) + self.W0  #  Your code (n x b)

    def backward(self, dLdZ):  
        # dLdZ is (n x b), uses stored self.A
        # store the derivatives for use in sgd_step and returd dLdA
        self.dLdW = np.dot(self.A,dLdZ.T)    # Your code
        ones = np.array([[1 for i in range(np.shape(dLdZ)[1])]])
        self.dLdW0 = np.dot(dLdZ,ones.T)      # Your code
        return np.dot(self.W,dLdZ)         # Your code: return dLdA (m x b)

    def sgd_step(self, lrate):  # Gradient descent step
        self.W = self.W-lrate*self.dLdW           # Your code
        self.W0 = self.W0-lrate*self.dLdW0           # Your code
    
